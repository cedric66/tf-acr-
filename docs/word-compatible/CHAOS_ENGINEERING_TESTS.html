<!DOCTYPE html>
<html><head><meta charset="UTF-8">
<style>
body { font-family: "Calibri", "Arial", sans-serif; font-size: 11pt; line-height: 1.5; color: #000; max-width: 800px; margin: 20px auto; }
h1 { font-size: 24pt; color: #2E74B5; border-bottom: 2px solid #2E74B5; padding-bottom: 10px; margin-top: 24pt; }
h2 { font-size: 18pt; color: #2E74B5; margin-top: 18pt; }
h3 { font-size: 14pt; color: #1F4D78; margin-top: 14pt; }
h4 { font-size: 12pt; font-weight: bold; margin-top: 12pt; }
p { margin-bottom: 10pt; }
ul, ol { margin-bottom: 10pt; }
li { margin-bottom: 4pt; }
pre { font-family: "Consolas", "Courier New", monospace; font-size: 10pt; background-color: #f5f5f5; padding: 10px; border: 1px solid #ddd; white-space: pre-wrap; word-wrap: break-word; }
code { font-family: "Consolas", "Courier New", monospace; color: #c7254e; background-color: #f9f2f4; padding: 2px 4px; border-radius: 4px; }
pre code { background-color: transparent; color: inherit; padding: 0; }
.mermaid { border: 1px solid #007acc; background-color: #e6f7ff; padding: 15px; border-radius: 5px; margin: 10px 0; }
.mermaid-title { color: #007acc; font-weight: bold; font-family: sans-serif; margin-bottom: 5px; }
table { border-collapse: collapse; width: 100%; margin-bottom: 15px; }
th, td { border: 1px solid #ddd; padding: 8px; text-align: left; }
th { background-color: #f2f2f2; color: #333; }
blockquote { border-left: 4px solid #ddd; padding-left: 15px; color: #666; font-style: italic; }
</style>
</head><body>
<h1>Chaos Engineering Test Scenarios for AKS Spot Node Optimization</h1>
<p><b>Document Version:</b> 1.0  </p>
<p><b>Created:</b> 2026-01-12  </p>
<p><b>Purpose:</b> Validate resilience of spot node architecture through controlled failure injection</p>
<p>---</p>
<h2>Test Framework Setup</h2>
<pre class="bash"><code>
# Install Chaos Mesh

kubectl create ns chaos-testing

helm repo add chaos-mesh https://charts.chaos-mesh.org

helm install chaos-mesh chaos-mesh/chaos-mesh -n chaos-testing



# Install LitmusChaos (alternative)

kubectl apply -f https://litmuschaos.github.io/litmus/litmus-operator-v2.0.0.yaml

</code></pre>
<p>---</p>
<h2>Failure Case 1: Simultaneous Multi-Pool Spot Eviction</h2>
<h3>Scenario Description</h3>
<p><b>Risk:</b> Azure evicts multiple spot pools simultaneously during high-demand periods  </p>
<p><b>Probability:</b> 0.5-2% during major Azure capacity events  </p>
<p><b>Impact:</b> Mass pod rescheduling, potential service degradation</p>
<h3>Test Objectives</h3>
<ul>
<li>Verify standard pool can absorb evicted workloads</li>
<li>Validate PodDisruptionBudgets maintain minimum availability</li>
<li>Measure time-to-recovery (TTR)</li>
<li>Confirm no request failures during eviction</li>
</ul>
<h3>Test 1.1: Coordinated Spot Pool Node Deletion</h3>
<pre class="yaml"><code>
# chaos-spot-multi-eviction.yaml

apiVersion: chaos-mesh.org/v1alpha1

kind: PodChaos

metadata:

  name: spot-multi-pool-eviction

  namespace: chaos-testing

spec:

  action: pod-kill

  mode: all

  duration: &quot;30s&quot;

  selector:

    namespaces:

      - production

    labelSelectors:

      &quot;kubernetes.azure.com/scalesetpriority&quot;: &quot;spot&quot;

  scheduler:

    cron: &quot;@every 1h&quot;

</code></pre>
<p><b>Execution:</b></p>
<pre class="bash"><code>
# Deploy test workload with 12 replicas

kubectl apply -f test-workloads/spot-tolerant-app.yaml



# Verify initial distribution

kubectl get pods -o wide | grep -E &quot;spotgen|stdworkload&quot;



# Inject chaos - kill all spot nodes simultaneously

kubectl apply -f chaos-spot-multi-eviction.yaml



# Monitor rescheduling

watch &quot;kubectl get pods -o wide | grep -E &#x27;Pending|ContainerCreating|Running&#x27;&quot;



# Check PDB status

kubectl get pdb -o wide



# Measure recovery time

kubectl get events --sort-by=&#x27;.lastTimestamp&#x27; | grep -E &quot;Scheduled|Started&quot;

</code></pre>
<p><b>Success Criteria:</b></p>
<ul>
<li>✅ Zero pods remain Pending >2 minutes</li>
<li>✅ Standard pool scales up within 90 seconds</li>
<li>✅ PDB maintains minAvailable throughout</li>
<li>✅ Application maintains >95% success rate (via metrics)</li>
</ul>
<p><b>Expected Behavior:</b></p>
<pre class=""><code>
T+0s:   Spot pods evicted (6 pods on spot, 6 on standard)

T+5s:   Pods marked Pending, scheduler places on standard nodes

T+15s:  Standard pool autoscaler triggers scale-up

T+90s:  New standard nodes ready

T+120s: All pods Running on standard nodes

</code></pre>
<h3>Test 1.2: Gradual Spot Pool Degradation</h3>
<pre class="yaml"><code>
# chaos-gradual-spot-loss.yaml

apiVersion: chaos-mesh.org/v1alpha1

kind: NodeChaos

metadata:

  name: gradual-spot-drain

  namespace: chaos-testing

spec:

  action: node-drain

  mode: fixed-percent

  value: &quot;33&quot;  # Drain 33% of spot nodes every iteration

  duration: &quot;10m&quot;

  selector:

    labelSelectors:

      &quot;kubernetes.azure.com/scalesetpriority&quot;: &quot;spot&quot;

  scheduler:

    cron: &quot;@every 5m&quot;

</code></pre>
<p><b>Success Criteria:</b></p>
<ul>
<li>✅ Workloads redistribute smoothly across remaining capacity</li>
<li>✅ No more than 1 pod disrupted per 30-second window</li>
<li>✅ Standard pool gradually absorbs load</li>
</ul>
<p>---</p>
<h2>Failure Case 2: Autoscaler Delay During Rapid Eviction</h2>
<h3>Scenario Description</h3>
<p><b>Risk:</b> Standard pool cannot scale fast enough to absorb sudden spot evictions  </p>
<p><b>Probability:</b> 10-15% during rapid eviction events  </p>
<p><b>Impact:</b> Pods stuck in Pending state, capacity shortage</p>
<h3>Test Objectives</h3>
<ul>
<li>Identify autoscaler response time bottlenecks</li>
<li>Validate overprovisioning strategy effectiveness</li>
<li>Test placeholder pod eviction mechanism</li>
</ul>
<h3>Test 2.1: Instant Capacity Demand Spike</h3>
<pre class="yaml"><code>
# chaos-instant-demand-spike.yaml

apiVersion: chaos-mesh.org/v1alpha1

kind: StressChaos

metadata:

  name: instant-demand-spike

  namespace: chaos-testing

spec:

  mode: all

  selector:

    namespaces:

      - production

    labelSelectors:

      &quot;kubernetes.azure.com/scalesetpriority&quot;: &quot;spot&quot;

  stressors:

    cpu:

      workers: 4

      load: 100

  duration: &quot;5m&quot;

---

# Simultaneously, kill spot nodes to force rescheduling

apiVersion: chaos-mesh.org/v1alpha1

kind: PodChaos

metadata:

  name: spot-instant-kill

  namespace: chaos-testing

spec:

  action: pod-kill

  mode: all

  selector:

    namespaces:

      - production

    labelSelectors:

      &quot;kubernetes.azure.com/scalesetpriority&quot;: &quot;spot&quot;

  duration: &quot;10s&quot;

</code></pre>
<p><b>Execution with Measurement:</b></p>
<pre class="bash"><code>
# Deploy overprovisioner pods (low priority)

kubectl apply -f cluster-overprovisioner.yaml



# Monitor autoscaler logs

kubectl logs -f -n kube-system -l app=cluster-autoscaler



# Inject chaos

kubectl apply -f chaos-instant-demand-spike.yaml



# Track pod scheduling latency

kubectl get events -w --field-selector reason=FailedScheduling



# Measure time from Pending to Running

for pod in $(kubectl get pods -l app=test -o name); do

  kubectl get $pod -o jsonpath=&#x27;{.status.conditions[?(@.type==&quot;PodScheduled&quot;)].lastTransitionTime}&#x27;

done

</code></pre>
<p><b>Success Criteria:</b></p>
<ul>
<li>✅ Overprovisioner pods evicted immediately (<5s)</li>
<li>✅ Real workloads schedule into freed capacity instantly</li>
<li>✅ Autoscaler provisions additional nodes within 2 minutes</li>
<li>✅ Maximum pending duration: <30 seconds</li>
</ul>
<h3>Test 2.2: Autoscaler Performance Under Load</h3>
<pre class="bash"><code>
# Stress test the autoscaler decision loop

# Deploy 100 pods simultaneously to trigger scaling

kubectl apply -f - &lt;&lt;EOF

apiVersion: apps/v1

kind: Deployment

metadata:

  name: autoscaler-stress-test

spec:

  replicas: 100

  selector:

    matchLabels:

      app: autoscaler-test

  template:

    metadata:

      labels:

        app: autoscaler-test

    spec:

      tolerations:

        - key: kubernetes.azure.com/scalesetpriority

          value: spot

          effect: NoSchedule

      containers:

        - name: pause

          image: gcr.io/google_containers/pause:3.1

          resources:

            requests:

              cpu: 500m

              memory: 512Mi

EOF



# Measure autoscaler scan intervals and decision time

kubectl logs -n kube-system -l app=cluster-autoscaler --tail=100 | grep &quot;scale up&quot;

</code></pre>
<p>---</p>
<h2>Failure Case 3: Spot VM Unavailability at Scale-Up Time</h2>
<h3>Scenario Description</h3>
<p><b>Risk:</b> No spot capacity available when autoscaler tries to provision nodes  </p>
<p><b>Probability:</b> 5-10% during peak demand hours  </p>
<p><b>Impact:</b> Pods remain pending, workloads run on expensive standard nodes</p>
<h3>Test Objectives</h3>
<ul>
<li>Verify priority expander fallback mechanism</li>
<li>Test multiple VM size diversity strategy</li>
<li>Validate cost alerts trigger appropriately</li>
</ul>
<h3>Test 3.1: Simulated Spot Capacity Exhaustion</h3>
<pre class="bash"><code>
# Manually cordon all spot nodes to simulate unavailability

for node in $(kubectl get nodes -l kubernetes.azure.com/scalesetpriority=spot -o name); do

  kubectl cordon $node

done



# Mark spot nodes as unschedulable

kubectl annotate nodes -l kubernetes.azure.com/scalesetpriority=spot \

  cluster-autoscaler.kubernetes.io/scale-down-disabled=true



# Deploy workload requiring spot

kubectl apply -f - &lt;&lt;EOF

apiVersion: apps/v1

kind: Deployment

metadata:

  name: test-spot-capacity

spec:

  replicas: 50

  selector:

    matchLabels:

      app: test-capacity

  template:

    metadata:

      labels:

        app: test-capacity

    spec:

      tolerations:

        - key: kubernetes.azure.com/scalesetpriority

          value: spot

          effect: NoSchedule

      affinity:

        nodeAffinity:

          preferredDuringSchedulingIgnoredDuringExecution:

            - weight: 100

              preference:

                matchExpressions:

                  - key: kubernetes.azure.com/scalesetpriority

                    operator: In

                    values: [spot]

      containers:

        - name: nginx

          image: nginx:alpine

          resources:

            requests:

              cpu: 250m

              memory: 256Mi

EOF



# Monitor where pods actually schedule

watch &quot;kubectl get pods -o wide | awk &#x27;{print \$7}&#x27; | sort | uniq -c&quot;

</code></pre>
<p><b>Success Criteria:</b></p>
<ul>
<li>✅ Pods schedule to standard nodes within 60 seconds</li>
<li>✅ No pods remain Pending for >90 seconds</li>
<li>✅ Cost monitoring alert fires for high standard usage</li>
<li>✅ Autoscaler logs show priority expander fallback</li>
</ul>
<p><b>Expected Logs:</b></p>
<pre class=""><code>
I0112 08:00:15 priority_expander.go:142] Priority expander: spot-general-1 (priority 10) - skip, no capacity

I0112 08:00:15 priority_expander.go:142] Priority expander: spot-general-2 (priority 10) - skip, no capacity

I0112 08:00:16 priority_expander.go:148] Priority expander: standard-workload (priority 20) - selected

</code></pre>
<h3>Test 3.2: VM Size Diversity Validation</h3>
<pre class="bash"><code>
# Verify different VM sizes reduce eviction correlation

# Get eviction events for each spot pool

kubectl get events --all-namespaces \

  --field-selector reason=Evicted \

  -o custom-columns=NODE:.source.host,TIME:.lastTimestamp | \

  grep -E &quot;spotgen1|spotgen2|spotcomp&quot; | \

  sort -k2



# Analyze correlation (should be independent)

# Expected: No timestamp clustering across different VM sizes

</code></pre>
<p>---</p>
<h2>Failure Case 4: Topology Spread Impossible Under Constraints</h2>
<h3>Scenario Description</h3>
<p><b>Risk:</b> <code>maxSkew</code> requirements cannot be satisfied due to capacity/zone constraints  </p>
<p><b>Probability:</b> 2-5% during zone capacity issues  </p>
<p><b>Impact:</b> Pods pending if using <code>DoNotSchedule</code>, or imbalanced distribution</p>
<h3>Test Objectives</h3>
<ul>
<li>Validate <code>whenUnsatisfiable: ScheduleAnyway</code> prevents deadlock</li>
<li>Test topology spread behavior under zone outage</li>
<li>Verify maxSkew violations trigger alerts</li>
</ul>
<h3>Test 4.1: Zone Failure Simulation</h3>
<pre class="yaml"><code>
# chaos-zone-failure.yaml

apiVersion: chaos-mesh.org/v1alpha1

kind: NetworkChaos

metadata:

  name: zone-partition

  namespace: chaos-testing

spec:

  action: partition

  mode: all

  selector:

    labelSelectors:

      &quot;topology.kubernetes.io/zone&quot;: &quot;australiaeast-1&quot;

  direction: both

  duration: &quot;10m&quot;

</code></pre>
<p><b>Execution:</b></p>
<pre class="bash"><code>
# Deploy workload with strict zone spread

kubectl apply -f - &lt;&lt;EOF

apiVersion: apps/v1

kind: Deployment

metadata:

  name: zone-spread-test

spec:

  replicas: 9  # 3 zones × 3 pods

  selector:

    matchLabels:

      app: zone-test

  template:

    metadata:

      labels:

        app: zone-test

    spec:

      topologySpreadConstraints:

        - maxSkew: 1

          topologyKey: topology.kubernetes.io/zone

          whenUnsatisfiable: ScheduleAnyway  # CRITICAL: Don&#x27;t block

          labelSelector:

            matchLabels:

              app: zone-test

      containers:

        - name: nginx

          image: nginx:alpine

EOF



# Inject zone failure

kubectl apply -f chaos-zone-failure.yaml



# Monitor distribution imbalance

kubectl get pods -o wide | \

  awk &#x27;{print $7}&#x27; | grep australiaeast | sort | uniq -c



# Check for skew violations

kubectl get events | grep &quot;FailedScheduling.*topology spread&quot;

</code></pre>
<p><b>Success Criteria:</b></p>
<ul>
<li>✅ Pods schedule despite zone outage (no Pending)</li>
<li>✅ Distribution skews to available zones (6-3-0 instead of 3-3-3)</li>
<li>✅ Alert fires for topology imbalance</li>
<li>✅ When zone recovers, pods rebalance automatically</li>
</ul>
<h3>Test 4.2: Impossible Constraints Deadlock Test</h3>
<pre class="yaml"><code>
# Test with DoNotSchedule to prove it blocks scheduling

apiVersion: apps/v1

kind: Deployment

metadata:

  name: deadlock-test

spec:

  replicas: 6

  selector:

    matchLabels:

      app: deadlock

  template:

    metadata:

      labels:

        app: deadlock

    spec:

      topologySpreadConstraints:

        - maxSkew: 0  # Impossible to satisfy

          topologyKey: kubernetes.io/hostname

          whenUnsatisfiable: DoNotSchedule  # BLOCKS scheduling

          labelSelector:

            matchLabels:

              app: deadlock

      containers:

        - name: nginx

          image: nginx:alpine

</code></pre>
<p><b>Expected Result:</b></p>
<pre class=""><code>
Events:

  Type     Reason            Message

  ----     ------            -------

  Warning  FailedScheduling  0/15 nodes available: 15 node(s) didn&#x27;t match pod topology spread constraints

</code></pre>
<p><b>Remediation Test:</b></p>
<pre class="bash"><code>
# Change to ScheduleAnyway

kubectl patch deployment deadlock-test -p &#x27;

spec:

  template:

    spec:

      topologySpreadConstraints:

        - maxSkew: 0

          topologyKey: kubernetes.io/hostname

          whenUnsatisfiable: ScheduleAnyway

          labelSelector:

            matchLabels:

              app: deadlock

&#x27;



# Pods should immediately schedule

kubectl get pods -l app=deadlock -o wide

</code></pre>
<p>---</p>
<h2>Failure Case 5: Graceful Shutdown Failure (30-Second Window)</h2>
<h3>Scenario Description</h3>
<p><b>Risk:</b> Application cannot complete graceful shutdown within Azure's 30-second eviction notice  </p>
<p><b>Probability:</b> 100% for long-running transactions or slow shutdown processes  </p>
<p><b>Impact:</b> Request failures, data inconsistency, poor user experience</p>
<h3>Test Objectives</h3>
<ul>
<li>Validate preStop hook execution</li>
<li>Test terminationGracePeriodSeconds configuration</li>
<li>Measure actual shutdown time</li>
<li>Verify zero request failures during shutdown</li>
</ul>
<h3>Test 5.1: Abrupt Pod Termination Test</h3>
<pre class="yaml"><code>
# chaos-abrupt-termination.yaml

apiVersion: chaos-mesh.org/v1alpha1

kind: PodChaos

metadata:

  name: abrupt-termination

  namespace: chaos-testing

spec:

  action: pod-kill

  mode: fixed

  value: &quot;1&quot;

  selector:

    namespaces:

      - production

    labelSelectors:

      &quot;app&quot;: &quot;api-service&quot;

  duration: &quot;1m&quot;

  gracePeriod: 5  # Shorter than app needs

</code></pre>
<p><b>Test Application (Slow Shutdown):</b></p>
<pre class="yaml"><code>
apiVersion: apps/v1

kind: Deployment

metadata:

  name: slow-shutdown-app

spec:

  replicas: 3

  selector:

    matchLabels:

      app: slow-shutdown

  template:

    metadata:

      labels:

        app: slow-shutdown

    spec:

      containers:

        - name: app

          image: nginxdemos/hello

          lifecycle:

            preStop:

              exec:

                command:

                  - /bin/sh

                  - -c

                  - |

                    echo &quot;Starting graceful shutdown...&quot;

                    # Simulate slow connection draining

                    sleep 40

                    echo &quot;Shutdown complete&quot;

          readinessProbe:

            httpGet:

              path: /

              port: 80

            periodSeconds: 1

      terminationGracePeriodSeconds: 45  # Longer than preStop

</code></pre>
<p><b>Execution:</b></p>
<pre class="bash"><code>
# Deploy slow shutdown app

kubectl apply -f slow-shutdown-app.yaml



# Monitor with load

kubectl run -it --rm load-generator --image=busybox -- sh -c \

  &quot;while true; do wget -q -O- http://slow-shutdown; sleep 0.1; done&quot;



# In another terminal, inject chaos

kubectl apply -f chaos-abrupt-termination.yaml



# Watch shutdown process

kubectl get events -w | grep -E &quot;Killing|Stopped&quot;



# Check if termination completes within grace period

kubectl logs -f &lt;pod-name&gt; --timestamps

</code></pre>
<p><b>Success Criteria:</b></p>
<ul>
<li>✅ preStop hook executes completely</li>
<li>✅ Pod marked NotReady immediately (removed from service endpoints)</li>
<li>✅ No new connections routed to terminating pod</li>
<li>✅ Existing connections complete before SIGKILL</li>
<li>✅ Load generator experiences <0.1% error rate</li>
</ul>
<h3>Test 5.2: Load Balancer Synchronization Test</h3>
<pre class="bash"><code>
# Verify pod removed from endpoints before shutdown

watch -n 0.5 &quot;kubectl get endpoints slow-shutdown -o json | jq &#x27;.subsets[].addresses | length&#x27;&quot;



# In parallel, kill a pod

kubectl delete pod &lt;pod-name&gt; --grace-period=30



# Expected: Endpoint count drops immediately, then pod terminates

</code></pre>
<p><b>Test Script:</b></p>
<pre class="bash"><code>
#!/bin/bash

# shutdown-timing-test.sh



POD_NAME=$(kubectl get pod -l app=slow-shutdown -o jsonpath=&#x27;{.items[0].metadata.name}&#x27;)



echo &quot;Starting shutdown timing test for $POD_NAME&quot;



# Mark start time

START=$(date +%s)



# Trigger deletion

kubectl delete pod $POD_NAME --grace-period=45 &amp;



# Monitor readiness

while kubectl get pod $POD_NAME 2&gt;/dev/null | grep -q Running; do

  READY=$(kubectl get pod $POD_NAME -o jsonpath=&#x27;{.status.conditions[?(@.type==&quot;Ready&quot;)].status}&#x27;)

  echo &quot;Pod ready status: $READY&quot;

  sleep 1

done



END=$(date +%s)

DURATION=$((END - START))



echo &quot;Total shutdown time: ${DURATION}s&quot;

[ $DURATION -lt 50 ] &amp;&amp; echo &quot;✅ PASS&quot; || echo &quot;❌ FAIL: Exceeded grace period&quot;

</code></pre>
<h3>Test 5.3: Connection Draining Validation</h3>
<pre class="python"><code>
# connection-draining-test.py

import requests

import time

import threading

from kubernetes import client, config



def make_requests(url, results):

    &quot;&quot;&quot;Continuously make requests and track failures&quot;&quot;&quot;

    while True:

        try:

            r = requests.get(url, timeout=2)

            results[&#x27;success&#x27;] += 1

        except:

            results[&#x27;failure&#x27;] += 1

        time.sleep(0.05)



def kill_pod(pod_name):

    &quot;&quot;&quot;Delete pod after 10 seconds&quot;&quot;&quot;

    time.sleep(10)

    config.load_kube_config()

    v1 = client.CoreV1Api()

    v1.delete_namespaced_pod(pod_name, &quot;production&quot;, grace_period_seconds=30)



# Start load

results = {&#x27;success&#x27;: 0, &#x27;failure&#x27;: 0}

load_thread = threading.Thread(target=make_requests, args=(&quot;http://slow-shutdown&quot;, results))

load_thread.daemon = True

load_thread.start()



# Kill pod after warmup

kill_thread = threading.Thread(target=kill_pod, args=(&quot;slow-shutdown-12345&quot;,))

kill_thread.start()



# Monitor for 60 seconds

for i in range(60):

    time.sleep(1)

    total = results[&#x27;success&#x27;] + results[&#x27;failure&#x27;]

    error_rate = (results[&#x27;failure&#x27;] / total * 100) if total &gt; 0 else 0

    print(f&quot;T+{i}s: Success={results[&#x27;success&#x27;]}, Failures={results[&#x27;failure&#x27;]}, Error Rate={error_rate:.2f}%&quot;)



# Expected: Error rate &lt;0.1% during shutdown window

</code></pre>
<p><b>Success Criteria:</b></p>
<ul>
<li>✅ Error rate <0.1% during shutdown</li>
<li>✅ All in-flight requests complete</li>
<li>✅ New requests route to healthy pods within 1 second</li>
</ul>
<p>---</p>
<h2>Test Execution Schedule</h2>
<table>
<tr><td>Week</td><td>Test Focus</td><td>Duration</td></tr>
<tr><td>1</td><td>Test setup + Failure Case 1</td><td>5 days</td></tr>
<tr><td>2</td><td>Failure Cases 2 & 3</td><td>5 days</td></tr>
<tr><td>3</td><td>Failure Cases 4 & 5</td><td>5 days</td></tr>
<tr><td>4</td><td>Full chaos scenario + report</td><td>5 days</td></tr>
</table>
<p>---</p>
<h2>Observability Requirements</h2>
<h3>Metrics to Track</h3>
<pre class="yaml"><code>
# ServiceMonitor for Prometheus

apiVersion: monitoring.coreos.com/v1

kind: ServiceMonitor

metadata:

  name: spot-optimization-metrics

spec:

  selector:

    matchLabels:

      app: spot-test

  endpoints:

    - port: metrics

      interval: 10s

</code></pre>
<p><b>Key Metrics:</b></p>
<ul>
<li><code>kube_pod_status_phase{phase="Pending"}</code> - Track pending pods</li>
<li><code>kube_node_status_condition{condition="Ready"}</code> - Node availability</li>
<li><code>cluster_autoscaler_scaled_up_nodes_total</code> - Scale-up events</li>
<li><code>http_request_duration_seconds</code> - Application latency</li>
<li><code>http_requests_total</code> - Success/failure rates</li>
</ul>
<h3>Dashboards</h3>
<pre class="bash"><code>
# Import Grafana dashboard for spot monitoring

kubectl apply -f - &lt;&lt;EOF

apiVersion: v1

kind: ConfigMap

metadata:

  name: spot-optimization-dashboard

  namespace: monitoring

data:

  spot-dashboard.json: |

    {

      &quot;dashboard&quot;: {

        &quot;title&quot;: &quot;Spot Node Optimization&quot;,

        &quot;panels&quot;: [

          {

            &quot;title&quot;: &quot;Pod Distribution by Node Type&quot;,

            &quot;targets&quot;: [{

              &quot;expr&quot;: &quot;count(kube_pod_info) by (node)&quot;

            }]

          },

          {

            &quot;title&quot;: &quot;Pending Pods&quot;,

            &quot;targets&quot;: [{

              &quot;expr&quot;: &quot;sum(kube_pod_status_phase{phase=&#x27;Pending&#x27;})&quot;

            }]

          }

        ]

      }

    }

EOF

</code></pre>
<p>---</p>
<h2>Automated Test Runner</h2>
<pre class="bash"><code>
#!/bin/bash

# chaos-test-runner.sh



set -e



echo &quot;=== AKS Spot Optimization Chaos Tests ===&quot;

echo &quot;Starting test suite at $(date)&quot;



# Test 1: Multi-pool eviction

echo &quot;Running Test 1: Multi-pool eviction...&quot;

kubectl apply -f chaos-spot-multi-eviction.yaml

sleep 300

kubectl delete -f chaos-spot-multi-eviction.yaml



# Test 2: Autoscaler delay

echo &quot;Running Test 2: Autoscaler delay...&quot;

kubectl apply -f chaos-instant-demand-spike.yaml

sleep 600

kubectl delete -f chaos-instant-demand-spike.yaml



# Test 3: Spot unavailability

echo &quot;Running Test 3: Spot capacity exhaustion...&quot;

./test-spot-capacity-exhaustion.sh



# Test 4: Topology spread

echo &quot;Running Test 4: Zone failure...&quot;

kubectl apply -f chaos-zone-failure.yaml

sleep 600

kubectl delete -f chaos-zone-failure.yaml



# Test 5: Graceful shutdown

echo &quot;Running Test 5: Shutdown timing...&quot;

./shutdown-timing-test.sh



echo &quot;=== All tests complete ===&quot;

echo &quot;Review results in test-results/$(date +%Y%m%d)&quot;

</code></pre>
<p>---</p>
<h2>Success Criteria Summary</h2>
<table>
<tr><td>Test</td><td>Metric</td><td>Target</td><td>Critical</td></tr>
<tr><td>Multi-pool eviction</td><td>Recovery time</td><td><120s</td><td>✅</td></tr>
<tr><td>Multi-pool eviction</td><td>Request success</td><td>>95%</td><td>✅</td></tr>
<tr><td>Autoscaler delay</td><td>Pending duration</td><td><30s</td><td>✅</td></tr>
<tr><td>Spot unavailability</td><td>Fallback time</td><td><60s</td><td>✅</td></tr>
<tr><td>Topology spread</td><td>Pod scheduling</td><td>100%</td><td>✅</td></tr>
<tr><td>Graceful shutdown</td><td>Error rate</td><td><0.1%</td><td>✅</td></tr>
</table>
<p>---</p>
<p><i>Document End</i></p>
</body></html>