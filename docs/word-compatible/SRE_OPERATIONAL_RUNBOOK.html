<!DOCTYPE html>
<html><head><meta charset="UTF-8">
<style>
body { font-family: "Calibri", "Arial", sans-serif; font-size: 11pt; line-height: 1.5; color: #000; max-width: 800px; margin: 20px auto; }
h1 { font-size: 24pt; color: #2E74B5; border-bottom: 2px solid #2E74B5; padding-bottom: 10px; margin-top: 24pt; }
h2 { font-size: 18pt; color: #2E74B5; margin-top: 18pt; }
h3 { font-size: 14pt; color: #1F4D78; margin-top: 14pt; }
h4 { font-size: 12pt; font-weight: bold; margin-top: 12pt; }
p { margin-bottom: 10pt; }
ul, ol { margin-bottom: 10pt; }
li { margin-bottom: 4pt; }
pre { font-family: "Consolas", "Courier New", monospace; font-size: 10pt; background-color: #f5f5f5; padding: 10px; border: 1px solid #ddd; white-space: pre-wrap; word-wrap: break-word; }
code { font-family: "Consolas", "Courier New", monospace; color: #c7254e; background-color: #f9f2f4; padding: 2px 4px; border-radius: 4px; }
pre code { background-color: transparent; color: inherit; padding: 0; }
.mermaid { border: 1px solid #007acc; background-color: #e6f7ff; padding: 15px; border-radius: 5px; margin: 10px 0; }
.mermaid-title { color: #007acc; font-weight: bold; font-family: sans-serif; margin-bottom: 5px; }
table { border-collapse: collapse; width: 100%; margin-bottom: 15px; }
th, td { border: 1px solid #ddd; padding: 8px; text-align: left; }
th { background-color: #f2f2f2; color: #333; }
blockquote { border-left: 4px solid #ddd; padding-left: 15px; color: #666; font-style: italic; }
</style>
</head><body>
<h1>SRE Operational Runbook: AKS Spot Node Management</h1>
<p><b>Document Owner:</b> SRE Team  </p>
<p><b>Created:</b> 2026-01-12  </p>
<p><b>Last Updated:</b> 2026-01-12  </p>
<p><b>On-Call Reference:</b> Priority 2</p>
<p>---</p>
<h2>Quick Reference</h2>
<h3>Emergency Contacts</h3>
<ul>
<li><b>Platform Team:</b> #platform-engineering (Slack), +61-XXX-XXX-XXX</li>
<li><b>Escalation:</b> Platform Lead (on-call rotation)</li>
<li><b>Incident Channel:</b> #incident-spot-nodes</li>
</ul>
<h3>Common Commands</h3>
<pre class="bash"><code>
# Check spot node status

kubectl get nodes -l kubernetes.azure.com/scalesetpriority=spot



# View pending pods

kubectl get pods -A --field-selector=status.phase=Pending



# Manual failover to standard

kubectl cordon -l kubernetes.azure.com/scalesetpriority=spot



# View eviction events (last hour)

kubectl get events -A --sort-by=&#x27;.lastTimestamp&#x27; | grep Evicted | tail -20



# Check autoscaler status

kubectl logs -n kube-system -l app=cluster-autoscaler --tail=50

</code></pre>
<p>---</p>
<h2>Overview</h2>
<h3>What is Spot Node Optimization?</h3>
<p>Spot nodes are Azure VMs offered at <b>60-90% discount</b> with the trade-off that Azure can evict them with <b>30 seconds notice</b> when capacity is needed.</p>
<p><b>Our Architecture:</b></p>
<ul>
<li><b>3 Spot Pools</b> (different VM sizes, different zones) - cost-optimized</li>
<li><b>1 Standard Pool</b> - fallback for evictions</li>
<li><b>1 System Pool</b> - cluster services (never spot)</li>
</ul>
<p><b>Target State:</b> 75% of workloads on spot, 25% on standard</p>
<p>---</p>
<h2>Monitoring & Alerting</h2>
<h3>Key Dashboards</h3>
<table>
<tr><td>Dashboard</td><td>URL</td><td>Purpose</td></tr>
<tr><td>Spot Optimization Overview</td><td>Grafana → AKS → Spot Overview</td><td>Health, costs, evictions</td></tr>
<tr><td>Pod Distribution</td><td>Grafana → AKS → Topology</td><td>Spread across pools</td></tr>
<tr><td>Autoscaler Status</td><td>Grafana → AKS → Autoscaler</td><td>Scale activity</td></tr>
<tr><td>Cost Trends</td><td>Azure Cost Management</td><td>Spend tracking</td></tr>
<h3>Critical Alerts</h3>
<table>
<tr><td>Alert</td><td>Severity</td><td>Response Time</td><td>Action</td></tr>
<tr><td><b>High Eviction Rate</b> (>20/hour)</td><td>P2</td><td>15 min</td><td>Investigate capacity issues</td></tr>
<tr><td><b>All Spot Pools Evicted</b></td><td>P1</td><td>5 min</td><td>Verify standard pool scaling</td></tr>
<tr><td><b>Pods Pending >5 min</b></td><td>P2</td><td>15 min</td><td>Check autoscaler logs</td></tr>
<tr><td><b>PDB Violations</b></td><td>P1</td><td>5 min</td><td>Emergency scale-up</td></tr>
<tr><td><b>Cost Spike</b> (>$200/day)</td><td>P3</td><td>1 hour</td><td>Review spot pricing</td></tr>
</table>
<p>---</p>
<h2>Runbooks</h2>
<h3>Runbook 1: High Eviction Rate Alert</h3>
<p><b>Alert:</b> <code>Spot eviction rate > 20 per hour</code></p>
<p><b>Cause:</b> Azure capacity demand spike in region</p>
<p><b>Impact:</b> Increased pod rescheduling, potential latency spikes</p>
<p><b>Response Procedure:</b></p>
<pre class="bash"><code>
# Step 1: Assess current eviction rate

kubectl get events -A --sort-by=&#x27;.lastTimestamp&#x27; | \

  grep -i evicted | \

  awk &#x27;{print $1}&#x27; | \

  uniq -c | \

  sort -rn



# Step 2: Check pod health

kubectl get pods -A | grep -E &quot;Pending|ContainerCreating|ImagePullBackOff&quot;



# Step 3: Verify standard pool capacity

kubectl get nodes -l priority=on-demand -o wide



# Step 4: Check autoscaler decisions

kubectl logs -n kube-system -l app=cluster-autoscaler --tail=100 | \

  grep -E &quot;scale|evict|spot&quot;



# Step 5: If standard pool not scaling, manually trigger

kubectl scale deployment &lt;deployment-name&gt; --replicas=&lt;current+2&gt;



# Step 6: Document in incident channel

# Post to #incident-spot-nodes with:

# - Current eviction rate

# - Affected namespaces

# - Standard pool status

# - Mitigation actions taken

</code></pre>
<p><b>Expected Resolution Time:</b> 5-10 minutes</p>
<p><b>Escalation:</b> If pods remain Pending >10 minutes, escalate to Platform Lead</p>
<p>---</p>
<h3>Runbook 2: All Spot Pools Evicted Simultaneously</h3>
<p><b>Alert:</b> <code>All spot node pools have zero ready nodes</code></p>
<p><b>Cause:</b> Major Azure capacity event (rare, ~1-2% probability)</p>
<p><b>Impact:</b> High - all spot workloads rescheduling to standard pool</p>
<p><b>Response Procedure:</b></p>
<pre class="bash"><code>
# Step 1: IMMEDIATE - Verify this is real eviction, not cluster issue

kubectl get nodes -l kubernetes.azure.com/scalesetpriority=spot



# Expected output: No nodes or all NotReady

# If nodes exist but NotReady, this is different issue - see Runbook 5



# Step 2: Check standard pool status

kubectl get nodes -l priority=on-demand



# Expected: Standard pool scaling up (NEW nodes in NotReady state)



# Step 3: Monitor pod rescheduling

watch &quot;kubectl get pods -A -o wide | grep -v Running | wc -l&quot;



# Expected: Decreasing count as pods schedule



# Step 4: Check PDB status - ensure we maintain minimums

kubectl get pdb -A



# Expected: All PDBs have ALLOWED &gt; 0



# Step 5: Verify application health

# Check your monitoring (e.g., Datadog, New Relic)

# Expected: Request success rate &gt;95%, latency &lt;2x baseline



# Step 6: Create incident ticket

# Priority: P1

# Title: &quot;All AKS spot pools evicted - failover to standard&quot;

# Include:

# - Time of eviction

# - Number of pods affected

# - Standard pool scale-up time

# - Application impact metrics

</code></pre>
<p><b>Expected Behavior:</b></p>
<ul>
<li>T+0s: Spot pools evicted</li>
<li>T+30s: All pods marked Pending</li>
<li>T+60s: Standard pool autoscaler triggers</li>
<li>T+180s: New standard nodes ready</li>
<li>T+240s: All pods Running on standard</li>
</ul>
<p><b>Escalation:</b> If standard pool fails to scale within 5 minutes, escalate to Platform Lead</p>
<p><b>Long-term Action:</b> Review spot pricing trends, consider adjusting VM sizes</p>
<p>---</p>
<h3>Runbook 3: Pods Stuck in Pending State</h3>
<p><b>Alert:</b> <code>Pending pods > 10 for > 5 minutes</code></p>
<p><b>Cause:</b> Multi-factor (capacity, scheduling constraints, resources)</p>
<p><b>Impact:</b> Application degradation, reduced capacity</p>
<p><b>Response Procedure:</b></p>
<pre class="bash"><code>
# Step 1: Identify why pods are pending

kubectl describe pod &lt;pending-pod-name&gt; | grep -A10 Events



# Common reasons and fixes:



## Reason 1: &quot;Insufficient CPU/memory&quot;

# Action: Verify autoscaler is attempting to scale

kubectl logs -n kube-system -l app=cluster-autoscaler --tail=50



## Reason 2: &quot;No nodes available matching nodeSelector&quot;

# Action: Check if spot pools are cordoned

kubectl get nodes | grep spot

# If cordoned, uncordon:

kubectl uncordon &lt;node-name&gt;



## Reason 3: &quot;Pod topology spread constraints not satisfied&quot;

# Action: This is expected during evictions, pods will schedule with skew

# Verify maxSkew allows &quot;ScheduleAnyway&quot;:

kubectl get deployment &lt;deployment&gt; -o yaml | grep -A5 topologySpreadConstraints



## Reason 4: &quot;spot pool has no capacity, standard pool scaling slowly&quot;

# Action: Check Azure portal for VM provisioning status

# OR manually add standard capacity:

az aks nodepool scale \

  --resource-group rg-aks-prod \

  --cluster-name aks-prod \

  --name stdworkload \

  --node-count &lt;current+3&gt;

</code></pre>
<p><b>Decision Tree:</b></p>
<pre class=""><code>
Pending Pods Detected

├─ Check Events

│  ├─ &quot;Insufficient resources&quot; → Verify autoscaler logs

│  ├─ &quot;No nodes match&quot; → Check node labels/taints

│  └─ &quot;Topology spread&quot; → Verify whenUnsatisfiable: ScheduleAnyway

├─ Duration &lt; 2 min → Monitor (normal during eviction)

├─ Duration 2-5 min → Check autoscaler activity

└─ Duration &gt; 5 min → Manual intervention required

</code></pre>
<p>---</p>
<h3>Runbook 4: Cost Spike Alert</h3>
<p><b>Alert:</b> <code>Daily AKS spend > $200 (threshold)</code></p>
<p><b>Cause:</b> Spot pools offline, workloads on expensive standard nodes</p>
<p><b>Impact:</b> Financial - reduced savings</p>
<p><b>Response Procedure:</b></p>
<pre class="bash"><code>
# Step 1: Check current node distribution

kubectl get nodes -o custom-columns=\

NAME:.metadata.name,\

PRIORITY:.metadata.labels.kubernetes\\.azure\\.com/scalesetpriority,\

SIZE:.metadata.labels.beta\\.kubernetes\\.io/instance-type



# Step 2: Count pods per node type

kubectl get pods -A -o json | \

jq -r &#x27;.items[] | .spec.nodeName&#x27; | \

xargs -I {} kubectl get node {} -o jsonpath=&#x27;{.metadata.labels.kubernetes\.azure\.com/scalesetpriority}{&quot;\n&quot;}&#x27; | \

sort | uniq -c



# Expected: 70-80% on spot

# If &lt;50% on spot, investigate



# Step 3: Check spot pricing

# Go to Azure Portal → Virtual Machines → Spot Pricing

# Review current spot prices vs on-demand



# Step 4: Decision matrix

if [ spot_price &gt; 0.5 * ondemand_price ]; then

  echo &quot;Spot pricing acceptable, should use spot&quot;

  # Investigate why workloads not on spot

  kubectl get nodes -l kubernetes.azure.com/scalesetpriority=spot

fi



# Step 5: If spot pools available but unused

# Check if pods have proper tolerations

kubectl get deployment &lt;deployment&gt; -o yaml | grep -A5 tolerations



# Step 6: Document in #finops-alerts channel

# Include:

# - Current daily spend

# - % of workloads on spot vs standard

# - Spot pricing trends

# - Recommended actions

</code></pre>
<p><b>Escalation:</b> If cost spike continues >24 hours, notify FinOps team</p>
<p>---</p>
<h3>Runbook 5: Node Not Ready After Eviction Recovery</h3>
<p><b>Alert:</b> <code>Node remains NotReady > 15 minutes after eviction</code></p>
<p><b>Cause:</b> Azure VM provisioning issue, kubelet crash, network issue</p>
<p><b>Impact:</b> Reduced capacity on spot pool</p>
<p><b>Response Procedure:</b></p>
<pre class="bash"><code>
# Step 1: Check node status

kubectl describe node &lt;node-name&gt; | grep -A20 Conditions



# Step 2: Check if node is in Azure

az vm list -g &lt;node-resource-group&gt; --query &quot;[?name==&#x27;&lt;node-name&gt;&#x27;]&quot;



# If node doesn&#x27;t exist in Azure:

## Azure failed to provision - this is normal for spot

## Autoscaler will retry, no action needed

## Monitor autoscaler logs:

kubectl logs -n kube-system -l app=cluster-autoscaler --tail=20



# If node exists in Azure but NotReady:

## SSH to node (if possible) and check kubelet

ssh azureuser@&lt;node-ip&gt;

sudo systemctl status kubelet

sudo journalctl -u kubelet --no-pager | tail -50



# Step 3: Common fixes

## Fix 1: Kubelet crash - restart

ssh azureuser@&lt;node-ip&gt; &quot;sudo systemctl restart kubelet&quot;



## Fix 2: Network issue - check CNI

kubectl get pods -n kube-system -o wide | grep &lt;node-name&gt;



## Fix 3: Node unresponsive - drain and delete

kubectl drain &lt;node-name&gt; --ignore-daemonsets --delete-emptydir-data

kubectl delete node &lt;node-name&gt;

# Autoscaler will replace



# Step 4: Monitor recovery

kubectl get node &lt;node-name&gt; -w

</code></pre>
<p>---</p>
<h3>Runbook 6: PodDisruptionBudget Violation</h3>
<p><b>Alert:</b> <code>PDB for <deployment> violated - healthy pods < minAvailable</code></p>
<p><b>Cause:</b> Too many simultaneous pod disruptions (evictions or rolling updates)</p>
<p><b>Impact:</b> CRITICAL - application below minimum availability</p>
<p><b>Response Procedure:</b></p>
<pre class="bash"><code>
# Step 1: IMMEDIATE - Identify affected deployment

kubectl get pdb -A

kubectl describe pdb &lt;pdb-name&gt; -n &lt;namespace&gt;



# Step 2: Check current pod status

kubectl get pods -n &lt;namespace&gt; -l app=&lt;app-label&gt;



# Count Running pods

kubectl get pods -n &lt;namespace&gt; -l app=&lt;app-label&gt; | grep Running | wc -l



# Step 3: IMMEDIATE MITIGATION

# Option A: Manually scale up deployment

kubectl scale deployment &lt;deployment&gt; -n &lt;namespace&gt; --replicas=&lt;current+3&gt;



# Option B: Pause autoscaler temporarily

kubectl annotate deployment &lt;deployment&gt; \

  cluster-autoscaler.kubernetes.io/safe-to-evict=false



# Step 4: Check for ongoing rollout

kubectl rollout status deployment/&lt;deployment&gt; -n &lt;namespace&gt;



# If rollout in progress and causing PDB violation:

kubectl rollout pause deployment/&lt;deployment&gt; -n &lt;namespace&gt;



# Step 5: Verify recovery

watch &quot;kubectl get pdb &lt;pdb-name&gt; -n &lt;namespace&gt;&quot;



# Expected: ALLOWED increases to &gt;0



# Step 6: Root cause analysis

# - Were pods evicted faster than they could be rescheduled?

# - Is minAvailable too aggressive?

# - Do we need more replicas?



# Document findings in incident report

</code></pre>
<p><b>Escalation:</b> IMMEDIATE escalation to Platform Lead and Application Owner</p>
<p>---</p>
<h2>Operational Workflows</h2>
<h3>Daily Operations Checklist</h3>
<p><b>Recommended:</b> Run at start of shift or via scheduled task</p>
<pre class="bash"><code>
#!/bin/bash

# daily-spot-check.sh



echo &quot;=== Daily AKS Spot Health Check ===&quot;

echo &quot;Date: $(date)&quot;



echo &quot;\n1. Node Status:&quot;

kubectl get nodes -o wide | grep -E &quot;NAME|spot&quot;



echo &quot;\n2. Eviction Count (last 24 hours):&quot;

kubectl get events -A --sort-by=&#x27;.lastTimestamp&#x27; | \

  grep -i evicted | \

  grep -E &quot;$(date +%Y-%m-%d)&quot; | \

  wc -l



echo &quot;\n3. Pending Pods:&quot;

kubectl get pods -A --field-selector=status.phase=Pending | wc -l



echo &quot;\n4. Pod Distribution:&quot;

kubectl get pods -A -o json | \

  jq -r &#x27;.items[] | select(.spec.nodeName != null) | .spec.nodeName&#x27; | \

  xargs -I {} kubectl get node {} -o jsonpath=&#x27;{.metadata.labels.kubernetes\.azure\.com/scalesetpriority}{&quot;\n&quot;}&#x27; 2&gt;/dev/null | \

  sort | uniq -c



echo &quot;\n5. Autoscaler Recent Activity:&quot;

kubectl logs -n kube-system -l app=cluster-autoscaler --tail=10 --since=1h



echo &quot;\n=== Check Complete ===&quot;

</code></pre>
<p><b>Expected Output:</b></p>
<ul>
<li>All spot nodes: Ready</li>
<li>Evictions: <50 per day</li>
<li>Pending pods: 0-2 (transient)</li>
<li>Distribution: 70-80% spot, 20-30% standard</li>
</ul>
<p>---</p>
<h3>Weekly Review</h3>
<p><b>Schedule:</b> Every Monday, 10:00 AM</p>
<p><b>Attendees:</b> SRE on-call, Platform Engineer, optional FinOps</p>
<p><b>Agenda:</b></p>
<ol>
<li><b>Review Metrics</b> (15 min)</li>
<ul>
<li>Total evictions last week</li>
<li>Average pod pending time</li>
<li>Cost variance vs budget</li>
<li>Spot adoption %</li>
</ul>
</ol>
<ol>
<li><b>Incident Review</b> (10 min)</li>
<ul>
<li>Any P1/P2 incidents related to spot</li>
<li>Lessons learned</li>
<li>Runbook updates needed</li>
</ul>
</ol>
<ol>
<li><b>Optimization Opportunities</b> (10 min)</li>
<ul>
<li>VM size adjustments</li>
<li>Workload re-classification (more to spot?)</li>
<li>Autoscaler tuning</li>
</ul>
</ol>
<ol>
<li><b>Action Items</b> (5 min)</li>
<ul>
<li>Assign owners</li>
<li>Set deadlines</li>
</ul>
</ol>
<p>---</p>
<h2>Key Metrics & SLOs</h2>
<h3>Service Level Objectives</h3>
<table>
<tr><td>SLO</td><td>Target</td><td>Measurement Window</td><td>Consequence of Miss</td></tr>
<tr><td><b>Availability</b></td><td>99.9%</td><td>30 days</td><td>Post-mortem required</td></tr>
<tr><td><b>Pod Scheduling Latency</b></td><td>P95 < 30s</td><td>7 days</td><td>Investigation required</td></tr>
<tr><td><b>Eviction Recovery</b></td><td>P99 < 120s</td><td>7 days</td><td>Runbook review</td></tr>
<tr><td><b>Cost Savings</b></td><td>>50% vs baseline</td><td>30 days</td><td>FinOps review</td></tr>
<h3>Metrics to Track</h3>
<pre class="promql"><code>
# Eviction rate (per hour)

rate(kube_pod_status_phase{phase=&quot;Failed&quot;, reason=&quot;Evicted&quot;}[1h])



# Pending pods count

count(kube_pod_status_phase{phase=&quot;Pending&quot;})



# Spot vs standard distribution

count(kube_pod_info) by (node)

# Join with node labels for spot/standard classification



# Pod scheduling latency

histogram_quantile(0.95,

  rate(scheduler_scheduling_duration_seconds_bucket[5m])

)



# Node readiness

kube_node_status_condition{condition=&quot;Ready&quot;, status=&quot;true&quot;}

</code></pre>
<p>---</p>
<h2>Troubleshooting Guide</h2>
<h3>Symptom: Slow Application Response Time</h3>
<p><b>Diagnosis:</b></p>
<pre class="bash"><code>
# Check if it coincides with eviction event

kubectl get events -A --sort-by=&#x27;.lastTimestamp&#x27; | head -20



# Check pod distribution - are all pods on one node?

kubectl get pods -n &lt;namespace&gt; -o wide



# Check node resources

kubectl top nodes

</code></pre>
<p><b>Likely Causes:</b></p>
<ol>
<li>Pod affinity caused all pods to land on single node during eviction</li>
<li>Insufficient replicas</li>
<li>Topology spread maxSkew too high</li>
</ol>
<p><b>Fix:</b></p>
<ul>
<li>Adjust topology spread constraints</li>
<li>Increase replica count</li>
<li>Review pod affinity rules</li>
</ul>
<p>---</p>
<h3>Symptom: Autoscaler Not Scaling Up Spot Pools</h3>
<p><b>Diagnosis:</b></p>
<pre class="bash"><code>
# Check autoscaler logs for errors

kubectl logs -n kube-system -l app=cluster-autoscaler | grep -i error



# Check if spot pool is at max capacity

kubectl get nodes -l kubernetes.azure.com/scalesetpriority=spot | wc -l

# Compare to max_count in Terraform



# Check Azure spot pricing

az vm list-skus --location australiaeast --size Standard_D --all --output table | grep Spot

</code></pre>
<p><b>Likely Causes:</b></p>
<ol>
<li>Spot pool at max_count - increase limit</li>
<li>No spot capacity in Azure - wait or use different VM size</li>
<li>Autoscaler disabled for pool - check configmap</li>
</ol>
<p><b>Fix:</b></p>
<ul>
<li>Increase max_count if intentionally limited</li>
<li>Wait for spot capacity (typically <30 mins)</li>
<li>Verify priority expander falls back to standard</li>
</ul>
<p>---</p>
<h2>Incident Response</h2>
<h3>P1 Incident: Multiple Application Outage Due to Eviction</h3>
<p><b>Immediate Actions (first 5 minutes):</b></p>
<ol>
<li>Join #incident-spot-nodes channel</li>
<li>Verify if this is spot-related:</li>
<pre class="bash"><code>
   kubectl get nodes -l kubernetes.azure.com/scalesetpriority=spot

</code></pre>
<li>Check affected applications</li>
<li>Manual scale-up standard pool if not auto-scaling:</li>
<pre class="bash"><code>
   az aks nodepool scale --resource-group rg-aks-prod \

     --cluster-name aks-prod --name stdworkload --node-count 15

</code></pre>
<li>Update status page</li>
</ol>
<p><b>Next Steps (5-30 minutes):</b></p>
<ul>
<li>Monitor pod rescheduling</li>
<li>Verify application recovery</li>
<li>Document timeline</li>
<li>Identify root cause</li>
</ul>
<p><b>Follow-up (after resolution):</b></p>
<ul>
<li>Conduct blameless post-mortem</li>
<li>Update runbooks</li>
<li>Consider architecture adjustments</li>
</ul>
<p>---</p>
<h2>On-Call Handoff</h2>
<h3>Information to Share</h3>
<p>When handing off on-call rotation:</p>
<ol>
<li><b>Active Issues</b></li>
<ul>
<li>Any ongoing eviction patterns</li>
<li>Pods that had prolonged pending states</li>
<li>Cost anomalies</li>
</ul>
</ol>
<ol>
<li><b>Recent Changes</b></li>
<ul>
<li>Terraform applied in last 24 hours</li>
<li>Deployments to production</li>
<li>Autoscaler config changes</li>
</ul>
</ol>
<ol>
<li><b>Monitoring Status</b></li>
<ul>
<li>Any flapping alerts (acknowledged but unresolved)</li>
<li>Known issues with dashboards</li>
</ul>
</ol>
<ol>
<li><b>Context</b></li>
<ul>
<li>Current spot adoption %</li>
<li>Recent incident history</li>
<li>Scheduled maintenance</li>
</ul>
</ol>
<p>---</p>
<h2>Tools & Resources</h2>
<h3>Essential Tools</h3>
<table>
<tr><td>Tool</td><td>Purpose</td><td>Access</td></tr>
<tr><td>kubectl</td><td>Cluster interaction</td><td>Local CLI</td></tr>
<tr><td>az CLI</td><td>Azure resources</td><td>Local CLI</td></tr>
<tr><td>Grafana</td><td>Monitoring dashboards</td><td>https://grafana.company.com</td></tr>
<tr><td>PagerDuty</td><td>Alerting</td><td>Mobile app</td></tr>
<tr><td>Slack</td><td>Communication</td><td>#platform-engineering, #sre</td></tr>
<h3>Reference Links</h3>
<ul>
<li><a href="https://docs.microsoft.com/en-us/azure/aks/">AKS Documentation</a></li>
<li><a href="https://docs.microsoft.com/en-us/azure/virtual-machines/spot-vms">Spot VMs Best Practices</a></li>
<li><a href="https://github.com/kubernetes/autoscaler/blob/master/cluster-autoscaler/FAQ.md">Cluster Autoscaler FAQ</a></li>
<li><a href="https://wiki.company.com/aks-architecture">Internal Wiki: AKS Architecture</a></li>
</ul>
<p>---</p>
<h2>Training & Onboarding</h2>
<h3>New SRE Checklist</h3>
<ul>
<li>[ ] Review this runbook</li>
<li>[ ] Review architecture document</li>
<li>[ ] Shadow existing SRE during on-call shift</li>
<li>[ ] Run through Runbooks 1-3 in dev environment</li>
<li>[ ] Access to all dashboards confirmed</li>
<li>[ ] PagerDuty escalation policy verified</li>
<li>[ ] Completed chaos engineering walkthrough</li>
</ul>
<p><b>Training Lab:</b> <code>dev-aks-cluster</code> has spot pools for practice.</p>
<p>---</p>
<p><b>Document Maintenance</b></p>
<p>This runbook should be reviewed and updated:</p>
<ul>
<li>After every P1/P2 incident involving spot nodes</li>
<li>Monthly during SRE team meeting</li>
<li>When architecture changes are deployed</li>
</ul>
<p><b>Last Review:</b> 2026-01-12  </p>
<p><b>Next Review Due:</b> 2026-02-12</p>
<p>---</p>
<p><b>On-Call Support</b></p>
<p>Questions? Contact:</p>
<ul>
<li><b>Platform Team:</b> #platform-engineering (Slack)</li>
<li><b>On-Call Engineer:</b> Check PagerDuty rotation</li>
<li><b>Emergency Escalation:</b> Platform Lead (see PagerDuty)</li>
</ul>
</body></html>